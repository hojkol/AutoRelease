import requests
import json
import csv
from datetime import datetime, timezone
import pandas as pd
from collections import OrderedDict
import yaml
import re



ITEM_HEADER = [
    'åŠŸèƒ½æ¨¡å—',
    'å‘å¸ƒç‰ˆæœ¬',
    'å‘ç‰ˆæ—¶é—´',
    'æ›´æ–°ç±»å‹',
    'ä¸€çº§åŠŸèƒ½',
    'äºŒçº§åŠŸèƒ½',
    'åŸºçº¿å‚æ•°'
]


# 1. è·å– tenant_access_token
def get_tenant_access_token(app_id, app_secret):
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal/"
    payload = {"app_id": app_id, "app_secret": app_secret}
    resp = requests.post(url, json=payload)
    return resp.json()["tenant_access_token"]

# 2. é€šè¿‡ node_token è·å–è¯¥çŸ¥è¯†ç©ºé—´èŠ‚ç‚¹ä¿¡æ¯ï¼Œæœ‰ apace_id, æŒ‚è½½çš„äº‘èµ„æºçš„ obj_token å’Œ obj_type
def get_node_info(node_token, tenant_access_token):
    url = "https://open.feishu.cn/open-apis/wiki/v2/spaces/get_node"
    headers = {"Authorization": f"Bearer {tenant_access_token}"}
    params = {"token": node_token, "obj_type": "wiki"}
    resp = requests.get(url, params=params, headers=headers)
    return resp.json()


# 3. è¯»å–è¡¨æ ¼å†…å®¹
def get_sheet_content(app_token, table_id, view_id, tenant_access_token, page_size=100, page_token=None):
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records/search"
    headers = {
        "Authorization": f"Bearer {tenant_access_token}",
        "Content-Type": "application/json"
    }
    payload = {
        "view_id": view_id,
        # å¯æ ¹æ®éœ€è¦æ·»åŠ ç­›é€‰æ¡ä»¶ filterã€æ’åºç­‰
    }
    resp = requests.post(url, headers=headers, data=json.dumps(payload))
    return resp.json()


def get_items(data):
    items = data['data']['items']
    
    filter_items = []
    # CSVè¡¨å¤´

    filter_items.append(ITEM_HEADER)
    for item in items:
        fields = item['fields']

        # å¤„ç†å¤šæ–‡æœ¬å­—æ®µï¼Œæå–"text"å¹¶ç”¨é€—å·è¿æ¥
        first_level = ','.join([x['text'] for x in fields.get('ä¸€çº§åŠŸèƒ½', [])]) if fields.get('ä¸€çº§åŠŸèƒ½') else ''
        second_level = ','.join([x['text'] for x in fields.get('äºŒçº§åŠŸèƒ½', [])]) if fields.get('äºŒçº§åŠŸèƒ½') else ''
        baseline = ','.join([x['text'] for x in fields.get('åŸºçº¿å‚æ•°', [])]) if fields.get('åŸºçº¿å‚æ•°') else ''
        # product_version = ','.join([x['text'] for x in fields.get('äº§å“æ¨¡å—ç‰ˆæœ¬', [])]) if fields.get('å‘å¸ƒç‰ˆæœ¬') else ''
        version = fields.get('ç‰ˆæœ¬', {})['value'][0]['text'] if fields.get('ç‰ˆæœ¬') else ''
        release_time = fields.get('å‘ç‰ˆæ—¶é—´', {})
        if release_time and 'value' in release_time and release_time['value']:
            # æ—¶é—´æˆ³å•ä½ä¸ºæ¯«ç§’ï¼Œè½¬æ¢ä¸ºç§’åè½¬æ—¥æœŸ
            timestamp = int(release_time['value'][0]) / 1000
            normal_date = datetime.fromtimestamp(timestamp, timezone.utc).strftime('%Y-%m-%d')
        else:
            normal_date = None
        row = [
            fields.get('åŠŸèƒ½æ¨¡å—', ''),
            version,
            normal_date,
            fields.get('æ›´æ–°ç±»å‹', ''),
            first_level,
            second_level,
            baseline
        ]
        # åªä¿ç•™ äº§å“æ¨¡å—ç‰ˆæœ¬ ä¸ä¸ºç©ºçš„è¡Œ
        if version:
            filter_items.append(row)

    # with open('output1.csv', mode='w', newline='', encoding='utf-8') as f:
    #     writer = csv.writer(f)
    #     for i in filter_items:
    #         writer.writerow(i)


    return filter_items


def get_release_Dict(data):
    # è½¬æˆDataFrame
    df = pd.DataFrame(data[1:], columns=data[0])

    def version_key(v):
        parts = v.lstrip('v').split('.')
        return tuple(int(x) for x in parts)

    df['ç‰ˆæœ¬æ’åº'] = df['å‘å¸ƒç‰ˆæœ¬'].apply(version_key)

    # è‡ªå®šä¹‰æ’åºæ˜ å°„
    module_order = {'ç®—åŠ›äº‘': 0, 'å¤§æ¨¡å‹æœåŠ¡å¹³å°': 1, 'è´¹ç”¨ä¸­å¿ƒ': 2, 'ç”¨æˆ·ä¸­å¿ƒ': 3}
    update_type_order = {'æ–°åŠŸèƒ½': 0, 'å¢å¼ºä¼˜åŒ–': 1, 'æ•…éšœä¿®å¤': 2, 'æ— æ›´æ–°ç±»å‹': 3}

    # æ›¿æ¢ç©ºæ›´æ–°ç±»å‹ä¸º'æ— æ›´æ–°ç±»å‹'
    df['æ›´æ–°ç±»å‹'] = df['æ›´æ–°ç±»å‹'].replace({'': 'æ— æ›´æ–°ç±»å‹'})

    # æ·»åŠ æ’åºè¾…åŠ©åˆ—
    df['åŠŸèƒ½æ¨¡å—æ’åº'] = df['åŠŸèƒ½æ¨¡å—'].map(module_order).fillna(99)
    df['æ›´æ–°ç±»å‹æ’åº'] = df['æ›´æ–°ç±»å‹'].map(update_type_order).fillna(99)

    # æŒ‰è¦æ±‚æ’åºï¼š
    # 1. å‘ç‰ˆæ—¶é—´é™åº
    # 2. åŠŸèƒ½æ¨¡å—è‡ªå®šä¹‰é¡ºåºå‡åº
    # 3. ç‰ˆæœ¬å·å‡åº
    # 4. æ›´æ–°ç±»å‹è‡ªå®šä¹‰é¡ºåºå‡åº
    # 5. ä¸€çº§åŠŸèƒ½å‡åº
    df_sorted = df.sort_values(by=['å‘ç‰ˆæ—¶é—´', 'åŠŸèƒ½æ¨¡å—æ’åº', 'ç‰ˆæœ¬æ’åº', 'æ›´æ–°ç±»å‹æ’åº', 'ä¸€çº§åŠŸèƒ½'],
                            ascending=[False, True, True, True, True])

    # æ„å»ºæœ‰åºå­—å…¸ç»“æ„
    result = OrderedDict()

    for _, row in df_sorted.iterrows():
        pub_date = row['å‘ç‰ˆæ—¶é—´']
        module = row['åŠŸèƒ½æ¨¡å—']
        version = row['å‘å¸ƒç‰ˆæœ¬']
        update_type = row['æ›´æ–°ç±»å‹']
        primary_func = row['ä¸€çº§åŠŸèƒ½']
        entry = {
            'äºŒçº§åŠŸèƒ½': row['äºŒçº§åŠŸèƒ½'],
            'åŸºçº¿å‚æ•°': row['åŸºçº¿å‚æ•°']
        }
        
        if pub_date not in result:
            result[pub_date] = OrderedDict()
        if module not in result[pub_date]:
            result[pub_date][module] = OrderedDict()
        if version not in result[pub_date][module]:
            result[pub_date][module][version] = OrderedDict()
        if update_type not in result[pub_date][module][version]:
            result[pub_date][module][version][update_type] = []
        
        result[pub_date][module][version][update_type].append((primary_func, entry))

    # ç»„å†…ä¸€çº§åŠŸèƒ½å†æ¬¡æ’åº
    for pub_date in result:
        for module in result[pub_date]:
            for version in result[pub_date][module]:
                for update_type in result[pub_date][module][version]:
                    result[pub_date][module][version][update_type].sort(key=lambda x: x[0])
    # # æ‰“å°ç»“æœç¤ºä¾‹
    # import pprint
    # pp = pprint.PrettyPrinter(indent=2, width=120)
    # pp.pprint(result)
    return result


def get_release_info(data, filename='rel-notes.md'):
    # result[å‘å¸ƒæ—¥æœŸ][åŠŸèƒ½æ¨¡å—][ç‰ˆæœ¬å·][æ›´æ–°ç±»å‹] = [(ä¸€çº§åŠŸèƒ½, {äºŒçº§åŠŸèƒ½, åŸºçº¿å‚æ•°}), ...]
    items = get_items(data)
    result = get_release_Dict(items)

    # æ›´æ–°ç±»å‹å¯¹åº”çš„ Emoji å’Œæ ‡é¢˜
    emoji_map = {
        'æ–°åŠŸèƒ½': 'ğŸš€ æ–°åŠŸèƒ½',
        'å¢å¼ºä¼˜åŒ–': 'âš¡ å¢å¼ºä¼˜åŒ–',
        'æ•…éšœä¿®å¤': 'ğŸ› æ•…éšœä¿®å¤',
        'æ— æ›´æ–°ç±»å‹': ''  # ç©ºæ›´æ–°ç±»å‹ä¸æ˜¾ç¤ºæ ‡é¢˜å’Œ Emoji
    }

    # ç”Ÿæˆ Markdown å†…å®¹
    lines = []
    lines.append('---')
    lines.append('hide:')
    lines.append('  - toc')
    lines.append('---\n')
    lines.append('# Release Notes\n')
    lines.append('æœ¬é¡µåˆ—å‡º d.run å„é¡¹åŠŸèƒ½çš„ä¸€äº›é‡è¦å˜æ›´ã€‚\n')

    # éå†æ—¥æœŸï¼ˆå·²æ’åºï¼‰
    for pub_date, modules in result.items():
        # lines.append(f'## {format_date(pub_date)}\n')
        lines.append(f'## {pub_date}\n')
        # éå†åŠŸèƒ½æ¨¡å—
        for module, versions in modules.items():
            # éå†ç‰ˆæœ¬å·
            for version, update_types in versions.items():
                lines.append(f'### {module} {version}\n')

                # æŒ‰å›ºå®šé¡ºåºè¾“å‡ºæ›´æ–°ç±»å‹
                for update_type in ['æ–°åŠŸèƒ½', 'å¢å¼ºä¼˜åŒ–', 'æ•…éšœä¿®å¤', 'æ— æ›´æ–°ç±»å‹']:
                    if update_type in update_types:
                        entries = update_types[update_type]
                        # éç©ºæ›´æ–°ç±»å‹è¾“å‡ºæ ‡é¢˜
                        if update_type != 'æ— æ›´æ–°ç±»å‹':
                            lines.append(f'#### {emoji_map[update_type]}\n')
                        # è¾“å‡ºæ¯æ¡è®°å½•ï¼Œæ ¼å¼ï¼š- [ä¸€çº§åŠŸèƒ½] åŸºçº¿å‚æ•°ï¼ˆè‹¥åŸºçº¿å‚æ•°ä¸ºç©ºåˆ™åªæ˜¾ç¤ºä¸€çº§åŠŸèƒ½ï¼‰
                        for primary_func, entry in entries:
                            baseline = entry['åŸºçº¿å‚æ•°']
                            if baseline:
                                lines.append(f'- [{primary_func}] {baseline}')
                            else:
                                lines.append(f'- [{primary_func}]')
                        lines.append('')  # æ¯ä¸ªæ›´æ–°ç±»å‹åç©ºè¡Œ

    md_content = '\n'.join(lines)

    # ä¿å­˜ä¸ºæ–‡ä»¶
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(md_content)



def parse_feishu_url(url):
    """
    Extract NODE_TOKEN, TABLE_ID, VIEW_ID from Feishu Bitable URL.
    """
    node_token = None
    table_id = None
    view_id = None

    # Extract the node token from the path (after /wiki/)
    match_node = re.search(r'/wiki/([^/?]+)', url)
    if match_node:
        node_token = match_node.group(1)

    # Extract query parameters table and view
    match_table = re.search(r'table=([^&]+)', url)
    if match_table:
        table_id = match_table.group(1)

    match_view = re.search(r'view=([^&]+)', url)
    if match_view:
        view_id = match_view.group(1)

    return node_token, table_id, view_id


def read_config_from_yaml(yaml_path):
    """
    Read config from YAML file, parse URL to get NODE_TOKEN, TABLE_ID, VIEW_ID,
    and return a config dictionary.
    """
    with open(yaml_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)

    app_id = data.get('APP_ID')
    app_secret = data.get('APP_SECRET')
    url = data.get('URL')

    node_token, table_id, view_id = parse_feishu_url(url) if url else (None, None, None)

    return app_id, app_secret, node_token, table_id, view_id



if __name__ == "__main__":

    app_path = 'APP.yaml'

    app_id, app_secret, node_token, table_id, view_id = read_config_from_yaml(app_path)

    tenant_access_token = get_tenant_access_token(app_id, app_secret)
    node_info = get_node_info(node_token, tenant_access_token)
    app_token = node_info["data"]["node"]["obj_token"]
    ori_data = get_sheet_content(app_token, table_id, view_id, tenant_access_token)
    # print(ori_data)
    get_release_info(ori_data)

